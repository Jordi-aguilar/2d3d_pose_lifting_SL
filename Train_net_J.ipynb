{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the imports from other files in the project\n",
    "from Model.DepthLSTM import DepthLSTM\n",
    "from Train.hyperparameters import *\n",
    "from Train.train_epoch_J import train_epoch_J\n",
    "from Train.test_epoch_J import test_epoch_J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train with preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_videos = './Data/Preprocessed_J/'\n",
    "list_videos = os.listdir(path_videos)\n",
    "videos_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in list_videos:\n",
    "    path_video = path_videos + video\n",
    "    videos_data.append(np.load(path_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_data[15].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_idx = [0, 4, 9, 12, 21]\n",
    "trainset_idx = list(set(range(24)) - set(testset_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = [videos_data[i] for i in trainset_idx]\n",
    "test_videos = [videos_data[i] for i in testset_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinVideos(videos_data):\n",
    "    span_videos = []\n",
    "    for video in videos_data:\n",
    "        span_vid = np.ones(len(video))\n",
    "        span_vid[0] = 0\n",
    "        span_videos.append(span_vid)\n",
    "    span_videos = np.concatenate(span_videos)\n",
    "    videos_data = np.concatenate(videos_data)\n",
    "    return videos_data, span_videos\n",
    "\n",
    "\n",
    "def reshapeBatches(vid_data, BATCH_SIZE, SEQ_LEN):\n",
    "    num_batches = vid_data.shape[0] // (BATCH_SIZE*SEQ_LEN)\n",
    "    cut_vid = vid_data[:num_batches*BATCH_SIZE*SEQ_LEN,]\n",
    "    reshaped_vid = cut_vid.reshape(BATCH_SIZE, num_batches, SEQ_LEN, -1)\n",
    "    reordered_vid = np.transpose(reshaped_vid, (1, 2, 0, 3))\n",
    "    \n",
    "    return reordered_vid\n",
    "\n",
    "\n",
    "def splitInputOutput(vid_data):\n",
    "    ground_idx = list(range(2,201,3))\n",
    "    input_idx = list(set(range(201)) - set(ground_idx))\n",
    "    \n",
    "    input_vid = vid_data[:,:,:,input_idx]\n",
    "    output_vid = vid_data[:,:,:,ground_idx]\n",
    "    \n",
    "    return input_vid, output_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stateful_dataset(videos, shuffle = True):\n",
    "    if shuffle:\n",
    "        random.shuffle(videos)\n",
    "        \n",
    "    videos_set, span_videos = joinVideos(videos)\n",
    "    \n",
    "    dataset = reshapeBatches(videos_set, BATCH_SIZE, SEQ_LEN)\n",
    "    span_videos = reshapeBatches(span_videos, BATCH_SIZE, SEQ_LEN)\n",
    "    \n",
    "    dataset, grounddataset = splitInputOutput(dataset)\n",
    "    \n",
    "    dataset = dataset.astype(np.float32)\n",
    "    grounddataset = grounddataset.astype(np.float32)\n",
    "    \n",
    "    return dataset, grounddataset, span_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_trainset, groundtrainset, span_videos_train = create_stateful_dataset(train_videos, shuffle = True)\n",
    "videos_testset, groundtestset, span_videos_test = create_stateful_dataset(test_videos, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_trainset.shape\n",
    "# [NUM BATCHES, SEQ_LEN, BATCH_SIZE, NUM_KEYPOINTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testset. Trainset is loaded at every epoch.\n",
    "testset, groundtestset, span_videos_test = create_stateful_dataset(train_videos, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST THE MODEL\n",
    "# Initialize the model\n",
    "model = DepthLSTM(HIDDEN_SIZE, NUM_LAYERS, SEQ_LEN, SEQ_LEN_TRAIN, BATCH_SIZE, NUM_JOINTS)\n",
    "model.to(device)\n",
    "initial_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    epoch = -1\n",
    "    model_name = f\"J_{epoch}.pt\"\n",
    "    model_path = \"./Output/\" + model_name\n",
    "    model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    initial_epoch = epoch+1\n",
    "    NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "tr_loss = []\n",
    "tst_loss = []\n",
    "state = None\n",
    "timer_beg = timer()\n",
    "# Train the model for NUM_EPOCHS epochs\n",
    "for epoch in range(initial_epoch, NUM_EPOCHS):\n",
    "    print('Starting epoch: ', epoch)\n",
    "    trainset, groundtrainset, span_videos_train = create_stateful_dataset(train_videos, shuffle = True)\n",
    "    train_epoch_loss, state = train_epoch_J(model, trainset, groundtrainset, span_videos_train, optimizer, loss_function)\n",
    "    test_epoch_loss, predY = test_epoch_J(model, testset, groundtestset, span_videos_test, loss_function)\n",
    "    timer_end = timer()  \n",
    "    if (epoch) % 10 == 0:\n",
    "        # Print the training loss of this epoch\n",
    "        # It is calculated as the average of losses of every window\n",
    "        print('Training loss in epoch {} is: {}'.format(epoch, sum(train_epoch_loss)/len(train_epoch_loss) ))\n",
    "        print('Test loss in epoch {} is: {}'.format(epoch, sum(test_epoch_loss)/len(test_epoch_loss) ))\n",
    "        \n",
    "        # Save model\n",
    "        \n",
    "    name = f\"J_{epoch}.pt\"\n",
    "    PATH = \"./Output/\" + name\n",
    "    torch.save(model, PATH)\n",
    "    \n",
    "    \n",
    "    with open(\"./Output/j_train_loss.txt\", 'a') as f:\n",
    "        f.write(str(sum(train_epoch_loss)/len(train_epoch_loss)) + '\\n')\n",
    "        \n",
    "    with open(\"./Output/j_test_loss.txt\", 'a') as f:\n",
    "        f.write(str(sum(test_epoch_loss)/len(test_epoch_loss)) + '\\n')\n",
    "    \n",
    "    if epoch > 1:\n",
    "        previous_model = f\"J_{epoch-1}.pt\"\n",
    "        os.system(f\"rm ./Output/{previous_model}\")\n",
    "\n",
    "    tr_loss.append(train_epoch_loss)\n",
    "    tst_loss.append(test_epoch_loss)\n",
    "    timer_beg = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(tr_loss).mean(axis = 1))\n",
    "plt.plot(np.array(tst_loss).mean(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"J_300.pt\"\n",
    "# PATH = \"./Output/\" + name\n",
    "# torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
